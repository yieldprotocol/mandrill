{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from getpass import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# import pdfkit\n",
    "import html\n",
    "from copy import deepcopy\n",
    "import re\n",
    "\n",
    "import re\n",
    "from tqdm.auto import tqdm \n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up an OpenAI template on the run\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "model.temperature = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task_creation_msgs = \\\n",
    "[\n",
    "    HumanMessage(content=\"I am building a city environment where a robot can interact. Can you define some set of actions a robot can perform in the environment? Please make sure to keep the number of actions as low as possible such that all other actions can be derived from the actions you define. Return the actions as a JSON string with keys as the action command and the description as the value.\"),\n",
    "    AIMessage(content=\\\n",
    "\"\"\"To define a minimalist set of actions a robot can perform in a city environment, we should aim for the most fundamental movements and interactions that other, more specific actions can be derived from. Here's a JSON representation of such actions:\n",
    "```json\n",
    "{\n",
    "  \"move_forward\": \"The robot advances in the direction it is currently facing.\",\n",
    "  \"turn_left\": \"The robot rotates 90 degrees to its left.\",\n",
    "  \"turn_right\": \"The robot rotates 90 degrees to its right.\",\n",
    "  \"pick_up\": \"The robot picks up an object within its reach.\",\n",
    "  \"put_down\": \"The robot places a held object down in the designated location.\",\n",
    "  \"interact\": \"The robot performs a general interaction with objects or interfaces in the environment (e.g., press a button, open a door).\"\n",
    "}\n",
    "```\n",
    "These actions are fundamental in the sense that combinations of movements (forward, left, right) can result in any navigational pattern within the city. The interaction commands (pick_up, put_down, interact) are abstract enough to encompass various object manipulations or interactions with the city environment. The actual implementation could be context-sensitive, meaning the exact action performed might depend on the robot's current state, position, and the characteristics of the object or interface it's interacting with.\"\"\"),\n",
    "    HumanMessage(content=\"Good, now create the environment overview that I should give the robot which can perform these actions in the environment. The environment description can be a natural language descripiton in a few sentences.\"),\n",
    "    AIMessage(content=\"The city environment is a grid-based layout with streets and sidewalks for navigation, various buildings, and diverse objects such as trash cans, benches, traffic signals, and streetlights. The robot operates primarily on the sidewalks but has to occasionally cross streets at designated crosswalks while obeying traffic signals. Buildings can occasionally be entered, with doorways that can be interacted with. There are areas designated for different activities, like parks with objects that can be picked up or put down, and zones marked for robot recharging. Other dynamic entities, such as pedestrians, vehicles, and animals, move in the environment and the robot must navigate around them, ensuring safe and respectful interaction.\"),\n",
    "    HumanMessage(content=\"Good, now please create some detailed tasks which the robot can perform in the environment using some combination of the actions. Note don't explain how to solve the task but only list down the feasible tasks in a sentence or two. Number each of the tasks you generate. Plz do not be vague in describing the task, describe every detail of the task.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_env_msgs = \\\n",
    "[\n",
    "    SystemMessage(content=\\\n",
    "\"\"\"You have to simulate a real environment to interact with an agent. You should simulate it by pretending to act as the environment. \n",
    "You have the following information:\n",
    "ENVIRONMENT OVERVIEW: Description of the environment.\n",
    "ACTIONS: list of actions you can perform in the environment.\n",
    "HISTORY: history of the agent's interaction with the environment.\n",
    "CURRENT STATE: current state of the environment as perceived by the agent.\n",
    "STOPPING CRITERIA: command that the environment prints when the task has been accomplished.\n",
    "\n",
    "To produce the ENVIRONMENT side of the conversation as a real environment you should think how the environment should behave. Please use the following format and think step by step:\n",
    "```\n",
    "## Thought: you should always think about what to do, explicitly restating the task without pronouns and restating details based on the conversation history and new input.\n",
    "## Environment: result of the agent's interaction with the environment.\n",
    "## Current State: after the previous action, describe the robot's view of the environment.\n",
    "```\"\"\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_agent_msgs = \\\n",
    "[\n",
    "    SystemMessage(content=\\\n",
    "\"\"\"\"You're a helpful AI agent. You have to interact with an environment to solve a task given by a user. To start with, you will be given the following information about the environment:\n",
    "ENVIRONMENT OVERVIEW: Description of the environment.\n",
    "ACTIONS: list of actions you can perform in the environment.\n",
    "HISTORY: history of the agent's interaction with the environment.\n",
    "CURRENT STATE: current state of the environment as perceived by the agent.\n",
    "STOPPING CRITERIA: command that the environment prints when the task has been accomplished.\n",
    "\n",
    "To solve a task, please use the following format and think step by step:\n",
    "```\n",
    "Thought: you should always think about what to do, explicitly restating the task without pronouns and restating details based on the conversation history and new input.\n",
    "Prior Observations: restate verbatim ALL details/names/figures/facts/etc from past observations relevant to the task and ALL related entities.\n",
    "Action: the action to perform in the environment. The action should obey the specified interaction format. The action should be in a JSON string, for example: \n",
    "```json\n",
    "{\n",
    "  Action: \"turn_left\"\"\n",
    "}\n",
    "``` \n",
    "```\"\"\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3e4fd2544240b4bfb6472c472ce5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_turns = 50\n",
    "all_convs = []\n",
    "for _ in tqdm(range(100)):\n",
    "    task_creation_msgs = _task_creation_msgs\n",
    "    response = model.predict_messages(task_creation_msgs)\n",
    "    task_creation_msgs.append(response)\n",
    "    task_creation_msgs.append(HumanMessage(content=\"Very good, now I will give the 2nd task for the robot to perform. Can you describe the state he is in, precisely describing the details of the environment with exact position (like sofa is on the left 10m apart, etc) and what the robot view at one instance.\"))\n",
    "    response = model.predict_messages(task_creation_msgs)\n",
    "    task_creation_msgs.append(response)\n",
    "    task_creation_msgs.append(HumanMessage(content=\"Can you imagine a stopping criteria when the robot succeeds in the given task? The stopping criteria should be a short statement/command produced by the environment when the robot completes the task.\"))\n",
    "    response = model.predict_messages(task_creation_msgs)\n",
    "    task_creation_msgs.append(response)\n",
    "    task_creation_msgs.append(HumanMessage(content=\\\n",
    "\"\"\"Very good, now I want to give the information I gathered from your help to an agent in a specific format. Can you please arrange it in the format described below. Pls just copy in the below format.\n",
    "## Environment Overview\n",
    "Description of the environment\n",
    "\n",
    "## Actions \n",
    "A JSON string to list all available set of actions\n",
    "\n",
    "## Task\n",
    "The task for the agent to perform\n",
    "\n",
    "## Current State\n",
    "The current state/location/position of the agent \n",
    "\n",
    "## Stopping Criteria\n",
    "A command that the environment prints when the task has been accomplished.\"\"\"))\n",
    "    response = model.predict_messages(task_creation_msgs)\n",
    "    env_setup_info = response.content\n",
    "    task = env_setup_info.split(\"## Task\")[-1].split(\"##\")[0].strip()\n",
    "    stop = env_setup_info.split(\"## Stopping Criteria\")[-1].strip()\n",
    "    \n",
    "    agent_msgs = _agent_msgs\n",
    "    convs = {\"conversations\": []}\n",
    "    agent_msgs.append(HumanMessage(content=env_setup_info))\n",
    "    agent_msgs.append(HumanMessage(content=f\"\"\"ENVIRONMENT: Here is your task. {task}\\n\\nAGENT:\"\"\"))\n",
    "    convs[\"conversations\"].append({\"from\": \"human\", \"value\": env_setup_info, \"loss\": False})\n",
    "    agent_response = model.predict_messages(agent_msgs)\n",
    "    agent_msgs.append(agent_response)\n",
    "    convs[\"conversations\"].append({\"from\": \"gpt\", \"value\": agent_response.content, \"loss\": True})\n",
    "    json_block = re.search(r\"```json(.*?)```\", agent_response.content, re.DOTALL)\n",
    "    json_str = json_block.group(1).strip()\n",
    "    \n",
    "    env_msgs = _env_msgs\n",
    "    env_msgs.append(HumanMessage(content=env_setup_info))\n",
    "    env_msgs.append(HumanMessage(content=f\"\"\"ENVIRONMENT: Here is your task. {task}\\n\\nAGENT: {json_str}\\n\\nENVIRONMENT:\"\"\"))\n",
    "    env_response = model.predict_messages(env_msgs)\n",
    "    convs[\"conversations\"].append({\"from\": \"human\", \"value\": env_response.content.split(\"## Environment:\")[-1].split(\"## Current State\")[0].strip(), \"loss\": False})\n",
    "    env_msgs.append(env_response)\n",
    "    agent_msgs.append(HumanMessage(content=f\"\"\"ENVIRONMENT: {env_response.content.split('## Environment:')[-1].split('## Current State')[0].strip()}\\n\\nAGENT:\"\"\"))\n",
    "    \n",
    "    for __ in range(num_turns):\n",
    "        agent_response = model.predict_messages(agent_msgs)\n",
    "        convs[\"conversations\"].append({\"from\": \"gpt\", \"value\": agent_response.content, \"loss\": True})\n",
    "        agent_msgs.append(agent_response)\n",
    "        json_block = re.search(r\"```json(.*?)```\", agent_response.content, re.DOTALL)\n",
    "        if json_block:\n",
    "            json_str = json_block.group(1).strip()\n",
    "        else:\n",
    "            json_str = \"No action needed further. Pls confirm if I have completed the task\"\n",
    "    \n",
    "        env_msgs.append(HumanMessage(content=f\"AGENT: {json_str}\\n\\nENVIRONMENT:\"))\n",
    "        env_response = model.predict_messages(env_msgs)\n",
    "        convs[\"conversations\"].append({\"from\": \"human\", \"value\": env_response.content.split(\"## Environment:\")[-1].split(\"## Current State\")[0].strip(), \"loss\": False})\n",
    "        env_msgs.append(env_response)\n",
    "        agent_msgs.append(HumanMessage(content=f\"\"\"ENVIRONMENT: {env_response.content.split('## Environment:')[-1].split('## Current State')[0].strip()}\\n\\nAGENT:\"\"\"))\n",
    "        \n",
    "        if stop in env_response.content:\n",
    "            break\n",
    "    all_convs.append(convs)\n",
    "    \n",
    "    with open('./synthetic-3/language_commands_syn.jsonl', 'w') as file:\n",
    "        for d in all_convs:\n",
    "            json.dump(d, file)\n",
    "            file.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
