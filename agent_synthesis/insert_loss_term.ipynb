{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at c:\\Users\\harsh\\Documents\\crypto-LLM-exps\\mandrill\\agent_synthesis\\synthetic-2\\dbbench-interactions-raw.jsonl\\dbbench-interactions-raw.jsonl.py or any data file in the same directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\harsh\\Documents\\crypto-LLM-exps\\mandrill\\agent_synthesis\\insert_loss_term.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/harsh/Documents/crypto-LLM-exps/mandrill/agent_synthesis/insert_loss_term.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39m./synthetic-2/dbbench-interactions-raw.jsonl\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1768\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[0;32m   1769\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1770\u001b[0m )\n\u001b[0;32m   1772\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1773\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1774\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   1775\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   1776\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1777\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1778\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1779\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1780\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1781\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1782\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1783\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1784\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1785\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1786\u001b[0m )\n\u001b[0;32m   1788\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1501\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m-> 1502\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[0;32m   1503\u001b[0m     path,\n\u001b[0;32m   1504\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1505\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1506\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1507\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1508\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1221\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1223\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at c:\\Users\\harsh\\Documents\\crypto-LLM-exps\\mandrill\\agent_synthesis\\synthetic-2\\dbbench-interactions-raw.jsonl\\dbbench-interactions-raw.jsonl.py or any data file in the same directory."
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ai2lumos/lumos_unified_ground_iterative\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(list):\n",
    "    new_list = []\n",
    "    for dict_ in list:\n",
    "        if dict_['role'] == 'assistant':\n",
    "            new_role = 'gpt'\n",
    "        else:\n",
    "            new_role = 'human'\n",
    "        new_list.append({'from': new_role, 'value': dict_['content']})\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('./synthetic-0/dbbench_simulated_hand_reviewed_219.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        ds = json.loads(line)\n",
    "        # ds = json.loads(line)['output']['history']\n",
    "        # ds = [{\"from\": \"human\" if dss['role']==\"user\" else \"gpt\", \"value\": dss['content']} for dss in ds]\n",
    "        data.append({\"conversations\": ds})\n",
    "# for ds in dataset:\n",
    "#     data.append({\"conversations\": map(ds[\"messages\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "some_ds = random.choices(data, k=5)\n",
    "for ds in data:\n",
    "    for j, turn in enumerate(ds['conversations']):\n",
    "        if len(some_ds[0]['conversations']) > j and len(some_ds[1]['conversations']) > j and len(some_ds[2]['conversations']) > j and len(some_ds[3]['conversations']) > j and len(some_ds[4]['conversations']) > j:\n",
    "            if turn['value']==some_ds[0]['conversations'][j][\"value\"] and \\\n",
    "                turn['value']==some_ds[1]['conversations'][j][\"value\"] and \\\n",
    "                turn['value']==some_ds[2]['conversations'][j][\"value\"] and \\\n",
    "                turn['value']==some_ds[3]['conversations'][j][\"value\"] and \\\n",
    "                turn['value']==some_ds[4]['conversations'][j][\"value\"]:\n",
    "                    ds['conversations'][j][\"loss\"] = None\n",
    "                    continue\n",
    "        if turn['from']=='human':\n",
    "            ds['conversations'][j][\"loss\"]= False\n",
    "            continue\n",
    "        ds['conversations'][j][\"loss\"]= True\n",
    "    new_data.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'I will ask you a question, then you should help me operate a MySQL database with SQL to answer the question. \\nYou have to explain the problem and your solution to me and write down your thoughts. After thinking and explaining \\nthoroughly, every round you can choose to operate or to answer. your operation should be like this: \\nAction: Operation ```sql SELECT * FROM table WHERE condition; ``` You MUST put SQL in markdown format \\nwithout any other comments. Your SQL should be in one line. Every time you can only execute one SQL statement. \\nI will only execute the statement in the first SQL code block. Every time you write a SQL, I will execute it for you \\nand give you the output. If the output is zero, or empty, you should always double check that you haven\\'t made a mistake before submitting a final answer.\\nYou can double check by removing limitations from your previous SQL statement until you get non-zero or non-empty results.\\nIf you are done operating, and you want to commit your final answer, then write down: \\nAction: Answer Final Answer: [\"ANSWER1\", \"ANSWER2\", ...] DO NOT write this pattern unless you are sure about your \\nanswer. You must ALWAYS provide SQL or a Final Answer. I expect an accurate and correct answer. Your answer should be accurate. \\nYour answer must be exactly the same as the correct answer. If the question is about modifying the database, then after you are done with operation, \\nyour answer field can be anything. If your response cannot match any pattern I mentioned earlier, \\nyou will be judged as FAIL immediately. Your input will be raw MySQL response, you have to deal with it by yourself.',\n",
       "   'loss': None},\n",
       "  {'from': 'gpt', 'value': 'Ok.', 'loss': None},\n",
       "  {'from': 'human',\n",
       "   'value': \"Query to find the total sales made by the employee 'John Doe'.. There are 2 tables involved in this task. The first table is named 'employees' with headers 'emp_id', 'first_name', 'last_name', 'email', 'phone_number'. The second table is named 'sales' with headers 'sale_id', 'emp_id', 'product_id', 'quantity', 'sale_date', 'total_price'. \",\n",
       "   'loss': False},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"To solve this problem, we need to join the two tables based on the common column, which is 'emp_id'. After joining, we'll filter the data to only include sales made by 'John Doe'. Finally, we'll sum up the 'total_price' of all these sales. Let's first check the 'employees' table to ensure 'John Doe' exists in it.\\n\\nAction: Operation\\n```sql\\nSELECT * FROM employees WHERE first_name = 'John' AND last_name = 'Doe';\\n```\",\n",
       "   'loss': True},\n",
       "  {'from': 'human',\n",
       "   'value': 'Query OK, 1 row affected \\n\\n+--------+------------+-----------+---------------------+--------------+\\n| emp_id | first_name | last_name | email               | phone_number |\\n+--------+------------+-----------+---------------------+--------------+\\n| 101    | John       | Doe       | john.doe@email.com  | 1234567890   |\\n+--------+------------+-----------+---------------------+--------------+\\n\\n',\n",
       "   'loss': False},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Great, we have confirmed that 'John Doe' exists in the 'employees' table and his 'emp_id' is 101. Now let's check the 'sales' table. We want to see if there are any sales associated with 'John Doe' (emp_id = 101).\\n\\nAction: Operation\\n```sql\\nSELECT * FROM sales WHERE emp_id = 101;\\n```\",\n",
       "   'loss': True},\n",
       "  {'from': 'human',\n",
       "   'value': 'Query OK, 3 rows affected \\n\\n+---------+--------+------------+----------+------------+-------------+\\n| sale_id | emp_id | product_id | quantity | sale_date  | total_price |\\n+---------+--------+------------+----------+------------+-------------+\\n| 1001    | 101    | 2001       | 10       | 2022-01-01 | 500.00      |\\n| 1002    | 101    | 2002       | 5        | 2022-01-02 | 250.00      |\\n| 1003    | 101    | 2003       | 7        | 2022-01-03 | 350.00      |\\n+---------+--------+------------+----------+------------+-------------+',\n",
       "   'loss': False},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Great, we have confirmed that there are sales associated with 'John Doe' (emp_id = 101) in the 'sales' table.\\n\\nNow, we can calculate the total sales made by 'John Doe' by summing up the 'total_price' from these sales.\\n\\nAction: Operation\\n```sql\\nSELECT SUM(total_price) FROM sales WHERE emp_id = 101;\\n```\",\n",
       "   'loss': True},\n",
       "  {'from': 'human',\n",
       "   'value': 'Query OK, 1 row affected \\n\\n+----------------+\\n| SUM(total_price) |\\n+----------------+\\n| 1100.00       |\\n+----------------+',\n",
       "   'loss': False},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Action: Answer\\nFinal Answer: [\"1100.00\"]',\n",
       "   'loss': True}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./synthetic-3/dbbench_simulated_hand_reviewed_219.jsonl', 'w') as file:\n",
    "    for conversation in new_data:\n",
    "        json_str = json.dumps(conversation)\n",
    "        file.write(json_str + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
