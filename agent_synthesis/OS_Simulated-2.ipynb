{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from getpass import getpass\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up an OpenAI template on the run\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "model.temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "all_data = []\n",
    "dataset = load_dataset(\"THUDM/AgentInstruct\", split=\"os\")\n",
    "for ds in dataset:\n",
    "    all_data.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_convs(s):\n",
    "    all_convs = []\n",
    "    s = s.split(\"## USER: Now, I will start a new problem in a new OS. My problem is:\")\n",
    "    s = [ss for ss in s if ss.strip()!=\"\" ]\n",
    "    s = [\"## USER: Now, I will start a new problem in a new OS. My problem is:\"+ss for ss in s]\n",
    "    for ss in s:\n",
    "        _dict = {\"conversations\": []}\n",
    "        for sss in ss.split(\"## USER:\")[1:]:\n",
    "            user_conv, agent_conv = sss.split('## AGENT:')[0].strip(), sss.split('## AGENT:')[1].strip()\n",
    "            _dict[\"conversations\"] += [{\"from\": \"human\", \"value\": user_conv, \"loss\": False}, {\"from\": \"gpt\", \"value\": agent_conv, \"loss\": True}]\n",
    "        all_convs.append(_dict)\n",
    "    return all_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import random\n",
    "num_turns = 1000\n",
    "init_prompt = \\\n",
    "\"\"\"\n",
    "Given is a conversations between an AI agent and a user. The user asks the agent to solve a task and in return the agent suggests the solution to that task. The agent produces bash commands which the user runs in his linux operating system and returns the output to the agent. You have to generate some more conversations following the previous. Pls be innovative in creating the task for the agent. \n",
    "\"\"\"\n",
    "all_convs = []\n",
    "all_tasks = []\n",
    "for _ in tqdm(range(num_turns)):\n",
    "    random.shuffle(all_data)\n",
    "    prompt = \"\"\n",
    "    for dd in all_data[0]['conversations'][:6]:\n",
    "        _from = \"USER\" if dd['from']==\"human\" else \"AGENT\"\n",
    "        prompt+=f\"\\n\\n## {_from}: {dd['value']}\"\n",
    "        if \"Now, I will start a new problem in a new OS. My problem is:\\n\\n\" in dd[\"value\"]:\n",
    "            all_tasks.append(dd[\"value\"].replace(\"Now, I will start a new problem in a new OS. My problem is:\\n\\n\", \"\"))\n",
    "    for d in all_data[1:7]:\n",
    "        for dd in d[\"conversations\"][6:]:\n",
    "            _from = \"USER\" if dd['from']==\"human\" else \"AGENT\"\n",
    "            prompt += f\"\\n\\n## {_from}: {dd['value']}\"\n",
    "            if \"Now, I will start a new problem in a new OS. My problem is:\\n\\n\" in dd[\"value\"]:\n",
    "                all_tasks.append(dd[\"value\"].replace(\"Now, I will start a new problem in a new OS. My problem is:\\n\\n\", \"\"))\n",
    "    task_messages =[\n",
    "        SystemMessage(content=init_task_gen_prompt),\n",
    "        HumanMessage(content=\"\\n\".join(all_tasks))\n",
    "    ]\n",
    "    messages = [\n",
    "        SystemMessage(content=init_prompt),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    try:\n",
    "        response = model.predict_messages(messages)\n",
    "        convs = convert_to_convs(response.content)\n",
    "        all_convs.extend(convs)    \n",
    "        messages.append(response)\n",
    "    except:\n",
    "        print(\"Not the assumed format!!\") \n",
    "        pass\n",
    "    try:\n",
    "        response = model.predict_messages(messages)\n",
    "        convs = convert_to_convs(response.content)\n",
    "        all_convs.extend(convs)\n",
    "        messages.append(response)\n",
    "    except:\n",
    "        print(\"Not the assumed format!!\") \n",
    "        pass    \n",
    "    try:\n",
    "        response = model.predict_messages(messages)\n",
    "        convs = convert_to_convs(response.content)\n",
    "        all_convs.extend(convs)    \n",
    "        messages.append(response)\n",
    "    except:\n",
    "        print(\"Not the assumed format!!\") \n",
    "        pass\n",
    "\n",
    "    with open('./synthetic-3/os_syn-datasyntype_2-2.json', 'w') as file:\n",
    "        json.dump(all_convs, file)\n",
    "        # for d in all_convs:\n",
    "        #     json.dump(d, file)\n",
    "        #     file.write('\\n')\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
